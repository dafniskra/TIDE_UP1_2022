{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data science pour l'assurance non vie, Partie 1\n",
    "### Krasniqi Dafnis, 06/10/2022, Master TIDE Paris 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Téléchargez les packages nécessaires à la réalisation du projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as geopd\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler, KBinsDiscretizer, FunctionTransformer, OneHotEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, auc\n",
    "\n",
    "from patsy import dmatrices, dmatrix\n",
    "\n",
    "import statsmodels\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install geapandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importer les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mtpl2(n_samples=100000):\n",
    "    \"\"\"Fetch the French Motor Third-Party Liability Claims dataset.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n_samples: int, default=100000\n",
    "      number of samples to select (for faster run time). Full dataset has\n",
    "      678013 samples.\n",
    "    \"\"\"\n",
    "    # freMTPL2freq dataset from https://www.openml.org/d/41214\n",
    "    df_freq = fetch_openml(data_id=41214, as_frame=True)['data']\n",
    "    df_freq['IDpol'] = df_freq['IDpol'].astype(int)\n",
    "    df_freq.set_index('IDpol', inplace=True)\n",
    "\n",
    "    # freMTPL2sev dataset from https://www.openml.org/d/41215\n",
    "    df_sev = fetch_openml(data_id=41215, as_frame=True)['data']\n",
    "\n",
    "    # sum ClaimAmount over identical IDs\n",
    "    df_sev = df_sev.groupby('IDpol').sum()\n",
    "\n",
    "    df = df_freq.join(df_sev, how=\"left\")\n",
    "    df[\"ClaimAmount\"].fillna(0, inplace=True)\n",
    "\n",
    "    # unquote string fields\n",
    "    for column_name in df.columns[df.dtypes.values == object]:\n",
    "        df[column_name] = df[column_name].str.strip(\"'\")\n",
    "    return df.iloc[:n_samples]\n",
    "  \n",
    "data = load_mtpl2(n_samples=678012)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. Analyser les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyser la base de données (nombre de ligne, les valeurs manquantes, ...)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyser la variable cible, indiquer la structure de cette variable\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de corrélation pour les variables quantitatives\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quelques modifications de la base de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plafoné les sinistres à 4\n",
    "data[\"ClaimNb\"] = \n",
    "\n",
    "\n",
    "#Plafonnement l'exposition des sinistres à 1\n",
    "data[\"Exposure\"] = \n",
    "\n",
    "\n",
    "#Plafonnement du Bonus-Malus à 150\n",
    "data[\"BonusMalus_capped\"]= \n",
    "\n",
    "\n",
    "#Plafonnement de l'âge du vehicule à 35\n",
    "data[\"VehAge_capped\"]= \n",
    "\n",
    "\n",
    "#Plafonnement de l'âge du conducteur à 85\n",
    "data[\"DrivAge_capped\"]= \n",
    "\n",
    "\n",
    "#Mettre la densité en log\n",
    "data['LogDensity'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Créez des classes pour les variables continues afin d'aider le modèle linéaire. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nous allons analyser l'âge du conducteur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot = data.groupby(data.DrivAge_capped).mean()\n",
    "data_plot[\"freq_2\"]=data_plot.ClaimNb/data_plot.Exposure\n",
    "\n",
    "# Plot the splines and error bands\n",
    "plt.scatter(data_plot.index, data_plot[\"freq_2\"] , facecolor='None', edgecolor='b', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.xlim(17,87)\n",
    "plt.ylim(0,0.4)\n",
    "plt.xlabel('DrivAge')\n",
    "plt.ylabel('freq')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Pour l'âge du conducteur, on crée des classes car la variable n'est pas linéaire. \n",
    "Voici les class: 18-25\n",
    "                 26-35\n",
    "                 36-50\n",
    "                 51-70\n",
    "                 71-80\n",
    "                 80 et plus''' \n",
    "\n",
    "def drive_age_class(data):\n",
    "    \n",
    "    drive_age = np.array(data[\"DrivAge_capped\"])\n",
    "\n",
    "    new_var=[]\n",
    "    for i in range(len(data)):\n",
    "        \n",
    "        if drive_age[i]<=25:\n",
    "            new_var.append('DrivAge_0')\n",
    "        elif drive_age[i]<=35 and drive_age[i]>25:\n",
    "            new_var.append('DrivAge_1')\n",
    "        elif drive_age[i]<=50 and drive_age[i]>35:\n",
    "            new_var.append('DrivAge_2')\n",
    "        elif drive_age[i]<=70 and drive_age[i]>50:\n",
    "            new_var.append('DrivAge_3')\n",
    "        elif drive_age[i]<=80 and drive_age[i]>70:\n",
    "            new_var.append('DrivAge_4')\n",
    "        else :\n",
    "            new_var.append(5)\n",
    "    \n",
    "    data[\"DrivAge_class\"]=new_var\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "data = drive_age_class(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyser la distribution de la variable nouvellement créée: \"DrivAge_class\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nous allons analyser l'âge de la voiture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot = data.groupby(data.VehAge_capped).mean()\n",
    "data_plot[\"freq_2\"]=data_plot.ClaimNb/data_plot.Exposure\n",
    "\n",
    "# Plot the splines and error bands\n",
    "plt.scatter(data_plot.index, data_plot[\"freq_2\"] , facecolor='None', edgecolor='b', alpha=0.6)\n",
    "plt.legend()\n",
    "plt.xlim(-2,87)\n",
    "plt.ylim(0,0.4)\n",
    "plt.xlabel('VehAge')\n",
    "plt.ylabel('freq')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Pour l'âge de la voiture, on crée des classes car la variable n'est pas linéaire. \n",
    "Voici les class: ????\n",
    "'''\n",
    "\n",
    "def veh_age_class(data):\n",
    "    \n",
    "    VehAge = np.array(data[\"VehAge_capped\"])\n",
    "\n",
    "    new_var=[]\n",
    "    \n",
    "    # A vous de jouer \n",
    "    \n",
    "    data[\"VehAge_class\"]=new_var\n",
    "    \n",
    "    return data\n",
    "\n",
    "data = veh_age_class(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyser la distribution de la variable nouvellement créée: \"VehAge_class\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faire des regroupements car il y a trop de modalités dans certaines variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nous allons nous intéresser aux marques des voitures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot = data.groupby(data.VehBrand).mean()\n",
    "data_plot[\"freq_2\"]=data_plot.ClaimNb/data_plot.Exposure\n",
    "\n",
    "plt.plot(data_plot.index, data_plot[\"freq_2\"], '.')\n",
    "\n",
    "plt.legend()\n",
    "plt.ylim(0,0.2)\n",
    "plt.xlabel('VehBrand')\n",
    "plt.ylabel('freq')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour marque de la voiture\n",
    "def VehBrand_class(data):\n",
    "\n",
    "    Brand = np.array(data[\"VehBrand\"])\n",
    "\n",
    "    VehBrand_classe = []\n",
    "    for i in range(0, len(data)):\n",
    "        \n",
    "        if Brand[i] in ['B2', 'B4', 'B6', 'B10', 'B1']:\n",
    "            VehBrand_classe.append('VehBrand_1')\n",
    "\n",
    "        elif Brand[i] in ['B3', 'B11', 'B5', 'B13', 'B14']:\n",
    "            VehBrand_classe.append('VehBrand_2')\n",
    "\n",
    "        else:\n",
    "            VehBrand_classe.append('VehBrand_3')\n",
    "\n",
    "    data[\"VehBrand_class\"]=VehBrand_classe\n",
    "        \n",
    "    return data\n",
    "\n",
    "data = VehBrand_class(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_plot = data.groupby(data.Area).mean()\n",
    "data_plot[\"freq_2\"]=data_plot.ClaimNb/data_plot.Exposure\n",
    "\n",
    "plt.plot(data_plot.index, data_plot[\"freq_2\"], '.')\n",
    "\n",
    "plt.legend()\n",
    "plt.ylim(0,0.2)\n",
    "plt.xlabel('Area')\n",
    "plt.ylabel('freq')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour la zone d'habitation\n",
    "data['Area'] = pd.to_numeric(data['Area'].map({\"A\": 1, \"B\": 2,\"C\": 3,\"D\": 4,\"E\": 5,\"F\": 6,}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour la region d'habitation\n",
    "gdf_france = geopd.read_file(\"regions-20140306-100m-shp/regions-20140306-100m.shp\")\n",
    "gdf_france = gdf_france[~gdf_france[\"nom\"].isin([\"Guadeloupe\", \"Guyane\", \"La Réunion\", \"Martinique\", \"Mayotte\"])]\n",
    "gdf_france['Region'] = \"R\" + gdf_france['code_insee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(9*2, 7))\n",
    "#fig.subplots_adjust(wspace=0.5)\n",
    "tmp = (data.groupby(['Region'])['Exposure', 'ClaimNb']\n",
    "          .sum()\n",
    "          .assign(frequency = lambda x: x['ClaimNb']/x['Exposure'])\n",
    "         )\n",
    "tmp = gdf_france.merge(tmp, on='Region', how='right')\n",
    "# https://stackoverflow.com/questions/38899190/geopandas-label-polygons\n",
    "tmp.plot(column='Exposure', legend=True, ax=axes[0], cmap='Greens')\n",
    "tmp.plot(column='frequency', legend=True, ax=axes[1], cmap='Greens')\n",
    "\n",
    "tmp.apply(lambda x: axes[0].annotate(s=x['Region'], xy=x.geometry.centroid.coords[0], ha='center'), axis=1);\n",
    "tmp.apply(lambda x: axes[1].annotate(s=x['Region'], xy=x.geometry.centroid.coords[0], ha='center'), axis=1);\n",
    "axes[0].set_title('Exposition par Region');\n",
    "axes[1].set_title(\"Fréquence per Region\");\n",
    "axes[0].set_xticklabels([])\n",
    "axes[0].set_yticklabels([])\n",
    "axes[1].set_xticklabels([])\n",
    "axes[1].set_yticklabels([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour la region d'habitation\n",
    "def Region_class(data):\n",
    "\n",
    "    Region = np.array(data[\"Region\"])\n",
    "\n",
    "    Region_classe = []\n",
    "    for i in range(0, len(data)):\n",
    "        \n",
    "    # A vous de jouer \n",
    "\n",
    "    data[\"Region_class\"]=Region_classe\n",
    "        \n",
    "    return data\n",
    "\n",
    "data = Region_class(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nous allons séparer notre base de données en une base de données d'entrainement et une base de données de test avec la fonction train_test_split  \n",
    "\n",
    "le test_size=0.2 et random_state=42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(data, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nous allons crée deux fonction pour juger de la qualité de nos modèles\n",
    "\n",
    "* $$GLM Poisson \\rightarrow \\mathcal{D} = 2* \\frac{1}{n}\\sum_{i=1}^{n}\\left(y_i\\log\\left(\\frac{y_i}{\\mu_i} \\right)  -(y_i-\\mu_i) \\right)$$\n",
    "\n",
    "\n",
    "* $$\\sum_{i \\in training} t_i y_i = \\sum_{i \\in training} t_i \\hat{\\mu}_i$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Deviance de Poisson\n",
    "def PD_function(pred, obs):\n",
    "    PD = 200*( sum(pred) - sum(obs) + sum( np.log( (obs/pred)**(obs) ) ) )\n",
    "    return PD/len(pred)\n",
    "\n",
    "# Function PD2: Print Poisson Deviance learn/test\n",
    "def PD2_function(txt, l_c, l_x, t_c, t_x):\n",
    "    print(\"{:s}, Train/test: {:.4f} / {:.4f}\".format(txt, PD_function(l_c, l_x), PD_function(t_c, t_x)))\n",
    "    \n",
    "    \n",
    "def diff_function(pred, obs):\n",
    "    return np.round(sum(pred)-sum(obs), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Création d'un dataframe pour enregistrer les résultats du test, du train et pour l'AIC. \n",
    "\n",
    "data_results_train = pd.DataFrame()\n",
    "data_results_test = pd.DataFrame()\n",
    "data_aic = dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modélisation\n",
    "\n",
    "## Modèle homogène\n",
    "\n",
    "Notre premier modèle est le plus simple, c'est-à-dire que nous allons donner la même fréquence d'accident à tout le monde. Pour ce faire, nous allons régresser uniquement sur l'intercept. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                ClaimNb   No. Observations:               542392\n",
      "Model:                            GLM   Df Residuals:                   542391\n",
      "Model Family:                 Poisson   Df Model:                            0\n",
      "Link Function:                    log   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:            -1.1762e+05\n",
      "Date:                Tue, 13 Sep 2022   Deviance:                   1.7974e+05\n",
      "Time:                        23:08:31   Pearson chi2:                 1.56e+06\n",
      "No. Iterations:                     7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     -2.2950      0.006   -390.186      0.000      -2.307      -2.283\n",
      "==============================================================================\n"
     ]
    }
   ],
   "source": [
    "GLM_poisson_homogene = smf.glm(\"ClaimNb ~ 1\",\n",
    "                               data=X_train, \n",
    "                               exposure=np.asarray(X_train['Exposure']),\n",
    "                               family=sm.families.Poisson(sm.genmod.families.links.log())\n",
    "                              ).fit()\n",
    "print(GLM_poisson_homogene.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La fréquence des accidents est:  0.10076389410011448\n"
     ]
    }
   ],
   "source": [
    "print(\"La fréquence des accidents est: \", np.exp(GLM_poisson_homogene.params[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les résultats doivent être enregistrés dans les dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_results_test['Homogene'] = GLM_poisson_homogene.predict(X_test[['Exposure']])* X_test[\"Exposure\"]\n",
    "data_results_train['Homogene'] = GLM_poisson_homogene.predict(X_train[['Exposure']])* X_train[\"Exposure\"]\n",
    "\n",
    "PD2_function(\"La déviance de Poisson pour le Modele Homogène est \", \n",
    "             data_results_train['Homogene'], X_train[\"ClaimNb\"], \n",
    "             data_results_test['Homogene'], X_test[\"ClaimNb\"])\n",
    "\n",
    "print('\\n')\n",
    "print(\"{:s}: {:.4f}\".format(\"L'aic est de\", GLM_poisson_homogene.aic)) \n",
    "data_aic['Homogene'] = GLM_poisson_homogene.aic\n",
    "\n",
    "print('\\n')\n",
    "print('la différence test', sum(data_results_test['Homogene']), sum(X_test[\"ClaimNb\"]))\n",
    "print('la différence train', sum(data_results_train['Homogene']), sum(X_train[\"ClaimNb\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modèle avec plusieurs variables de régression \n",
    "\n",
    "Pour réaliser cette exercice, il faut utiliser dmatrices. Voici un exemple:\n",
    "\n",
    "formule = \"ClaimNb ~ C(DrivAge_class)\"\n",
    "\n",
    "response, predictors_train = dmatrices(formule, X_train, return_type='dataframe')\n",
    "\n",
    "Glm_Divers = sm.GLM(response_train, predictors_train,\n",
    "              exposure=np.asarray(X_train.Exposure),\n",
    "              family=sm.families.Poisson(sm.genmod.families.links.log())).fit()\n",
    "\n",
    "print(Glm_Divers.summary())\n",
    "\n",
    "\n",
    "S'il y a une variable catégorielle, vous devez ajouter un C et mettre la variable entre parenthèses. La fonction dmatrices va créer automatiquement la base de données en effectuant directement l'auto-codage pour les variables catégorielles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A vous de jouer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les résultats doivent être enregistrés dans les dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A vous de jouer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qu'est-ce qui se passe avec la déviance de Poisson et l'AIC ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réponse:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3. Modèle avec plusieurs variables de régression et une étape de sélection des variables\n",
    "\n",
    "Comme il est extrêmement difficile de calibrer le modèle à la main, nous procéderons par étapes. Nous coderons une méthode par étapes pour sélectionner les variables les plus pertinentes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stepwise_reg(data_train, candidat):\n",
    "    \n",
    "    save_model = []\n",
    "\n",
    "    GLM_select = smf.glm(\"ClaimNb ~ 1\",\n",
    "                         data=data_train,\n",
    "                         exposure=np.asarray(data_train['Exposure']),\n",
    "                         family=sm.families.Poisson(sm.genmod.families.links.log())).fit()\n",
    "\n",
    "    score_aic = GLM_select.aic\n",
    "    iteration=0\n",
    "\n",
    "    while len(candidat)>0:\n",
    "\n",
    "        #stacker les resultats\n",
    "        save_result = pd.DataFrame(columns=['candidat','aic'])\n",
    "        print(\"l'aic de référence est :\", score_aic)\n",
    "        #print('les variables candidates pour le modèle sont :', candidat)\n",
    "\n",
    "        for c in candidat:\n",
    "\n",
    "            iteration = iteration+1\n",
    "\n",
    "            model = ' + '.join(save_model + [c])\n",
    "            \n",
    "            formula = \"\"\"ClaimNb ~ \"\"\" + model\n",
    "\n",
    "            response, predictors = dmatrices(formula, data_train, return_type='dataframe')\n",
    "\n",
    "            GLM_select = sm.GLM(response, predictors, exposure=np.asarray(data_train['Exposure']),\n",
    "                                family=sm.families.Poisson(sm.genmod.families.links.log())).fit()\n",
    "\n",
    "            print(\"iter: \"+str(iteration)+\", candidat: \"+str(c)+\", aic: \"+str(GLM_select.aic))\n",
    "\n",
    "            save_result = save_result.append({'candidat':c, 'aic':GLM_select.aic}, ignore_index=True)\n",
    "\n",
    "        index = save_result['aic'].idxmin()\n",
    "\n",
    "        if score_aic<save_result['aic'][index]:\n",
    "\n",
    "            print('breakkkkkkkkk')\n",
    "\n",
    "            break\n",
    "\n",
    "        else:\n",
    "            score_aic=save_result['aic'][index]\n",
    "\n",
    "        save_model.append(save_result['candidat'][index])\n",
    "        print('\\n')\n",
    "        print('la meilleure variable est: '+ str(save_result['candidat'][index]))\n",
    "        print('avec un aic de: ' + str(save_result['aic'][index]))\n",
    "        print('\\n')\n",
    "        candidat.remove(save_result['candidat'][index])                  \n",
    "\n",
    "    print(\"le meilleur modele est :\", save_model)\n",
    "    \n",
    "    return save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sélectionner les meilleures variables \n",
    "candidat = [\"C(Area)\", \"VehPower\",\"LogDensity\", \"C(DrivAge_class)\", \"C(VehAge_class)\",\n",
    "            \"C(VehBrand_class)\", \"Area\", \"C(Region_class)\", \"BonusMalus_capped\"]\n",
    "\n",
    "best_var = stepwise_reg(X_train, candidat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utiliser les meilleures variables pour construire un nouveau modèle. \n",
    "model = ' + '.join(best_var)\n",
    "\n",
    "formula = \"\"\"ClaimNb ~ \"\"\" + model\n",
    "response_train, predictors_train  = dmatrices(formula, X_train, return_type='dataframe')\n",
    "response_test, predictors_test = dmatrices(formula, X_test, return_type='dataframe')\n",
    "\n",
    "GLM_step_wise = sm.GLM(response_train, predictors_train, exposure=np.asarray(X_train['Exposure']),\n",
    "                                    family=sm.families.Poisson(sm.genmod.families.links.log())).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_results_test['step_wise'] = GLM_step_wise.predict(predictors_test)* X_test[\"Exposure\"]\n",
    "data_results_train['step_wise'] = GLM_step_wise.predict(predictors_train)* X_train[\"Exposure\"]\n",
    "\n",
    "PD2_function(\"La déviance de Poisson pour le Modele stepwise est \", \n",
    "             data_results_train['step_wise'], X_train[\"ClaimNb\"], \n",
    "             data_results_test['step_wise'], X_test[\"ClaimNb\"])\n",
    "\n",
    "print('\\n')\n",
    "print(\"{:s}: {:.4f}\".format(\"L'aic est de\", GLM_step_wise.aic)) \n",
    "data_aic['step_wise'] = GLM_step_wise.aic\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "print('la différence test', sum(data_results_test['step_wise']), sum(X_test[\"ClaimNb\"]))\n",
    "print('la différence train', sum(data_results_train['step_wise']), sum(X_train[\"ClaimNb\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generalized Additive Model (GAM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prenons l'exemple de l'âge du conducteur. Dans la section précédente, nous avons discrétisé cette variable mais nous pourrions faire mieux en utilisant des splines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df=11 c'est le nombre de degré de liberté, il peut etre selectionné avec une cross validation\n",
    "transformed_DrivAge = dmatrix(\"cr(X_train.DrivAge_capped, df = 7)\", \n",
    "                         {\"train\": X_train.DrivAge_capped}, \n",
    "                         return_type='dataframe')\n",
    "\n",
    "GAM_DrivAge = sm.GLM(X_train.ClaimNb, transformed_DrivAge,\n",
    "              exposure=np.asarray(X_train.Exposure),\n",
    "              family=sm.families.Poisson(sm.genmod.families.links.log())).fit()\n",
    "\n",
    "print(GAM_DrivAge.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mat = X_train.groupby(X_train.DrivAge_capped).mean()\n",
    "Mat[\"freq_2\"]=Mat.ClaimNb/Mat.Exposure\n",
    "pred1 = GAM_DrivAge.predict(dmatrix(\"cr(age_grid, df = 7)\", \n",
    "                             {\"age_grid\": Mat.index}, return_type='dataframe'))\n",
    "\n",
    "# Plot the splines and error bands\n",
    "plt.scatter(Mat.index, Mat[\"freq_2\"] , facecolor='None', edgecolor='k', alpha=0.6)\n",
    "plt.plot(Mat.index, pred1, color='r', label='splines cubiques naturelles')\n",
    "plt.legend()\n",
    "plt.xlim(17,87)\n",
    "plt.ylim(0,0.4)\n",
    "plt.xlabel('DrivAge')\n",
    "plt.ylabel('freq')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Mat.index\n",
    "y = dmatrix(\"cr(x, df = 7) - 1\", {\"x\": x})\n",
    "b=np.array(GAM_DrivAge.params[1:])\n",
    "plt.title(\"splines cubiques naturelles\");\n",
    "plt.scatter(Mat.index, Mat[\"freq_2\"] , facecolor='None', edgecolor='k', alpha=0.6)\n",
    "plt.plot(Mat.index, np.exp(np.array(GAM_DrivAge.params[0]))*np.exp(y*b))\n",
    "plt.xlabel('DrivAge')\n",
    "plt.ylabel('freq')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C'est à vous de jouer pour la variable âge de la voiture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GAM âge de la voiture\n",
    "## # A vous de jouer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Etape de sélection des variables avec les splines\n",
    "\n",
    "Nous allons faire la même chose que précédemment mais cette fois-ci nous allons rajouter la variable d'âge que nous avons construite avec les splines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sélectionner les meilleures variables \n",
    "candidat_spline = [\"C(Area)\", \"VehPower\",\"LogDensity\", \"cr(DrivAge_capped, df = 7)\", \"C(VehAge_class)\",\n",
    "            \"C(VehBrand_class)\", \"Area\", \"C(Region_class)\", \"BonusMalus_capped\"]\n",
    "\n",
    "best_var_spline = stepwise_reg(X_train, candidat_spline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utiliser les meilleures variables pour construire un nouveau modèle. \n",
    "var_spline = ' + '.join(best_var_spline)\n",
    "\n",
    "formula = \"\"\"ClaimNb ~ \"\"\" + var_spline\n",
    "response_train, predictors_train  = dmatrices(formula, X_train, return_type='dataframe')\n",
    "response_test, predictors_test = dmatrices(formula, X_test, return_type='dataframe')\n",
    "\n",
    "GLM_step_wise_spline = sm.GLM(response_train, predictors_train, exposure=np.asarray(X_train['Exposure']),\n",
    "                                    family=sm.families.Poisson(sm.genmod.families.links.log())).fit()\n",
    "\n",
    "print(GLM_step_wise_spline.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_results_test['step_wise_spline'] = GLM_step_wise_spline.predict(predictors_test)* X_test[\"Exposure\"]\n",
    "data_results_train['step_wise_spline'] = GLM_step_wise_spline.predict(predictors_train)* X_train[\"Exposure\"]\n",
    "\n",
    "PD2_function(\"La déviance de Poisson pour le Modele stepwise avec spline est \", \n",
    "             data_results_train['step_wise_spline'], X_train[\"ClaimNb\"], \n",
    "             data_results_test['step_wise_spline'], X_test[\"ClaimNb\"])\n",
    "\n",
    "print('\\n')\n",
    "print(\"{:s}: {:.4f}\".format(\"L'aic est de\", GLM_step_wise_spline.aic)) \n",
    "data_aic['step_wise_spline'] = GLM_step_wise_spline.aic\n",
    "\n",
    "print('\\n')\n",
    "print('la différence test', sum(data_results_test['step_wise_spline']), sum(X_test[\"ClaimNb\"]))\n",
    "print('la différence train', sum(data_results_train['step_wise_spline']), sum(X_train[\"ClaimNb\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résumé"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in data_results_test.columns:\n",
    "    print(i)\n",
    "    print(\"- Pour le Modele \"+str(i)+\" :\")\n",
    "    PD2_function(\"La déviance de Poisson est \", \n",
    "             data_results_train[i], X_train[\"ClaimNb\"], \n",
    "             data_results_test[i], X_test[\"ClaimNb\"])\n",
    "    \n",
    "    print(\"- l'aic est de \" +str(data_aic[i]))\n",
    "    print('- la différence test', sum(data_results_test[i]), sum(X_test[\"ClaimNb\"]))\n",
    "    print('- la différence train', sum(data_results_train[i]), sum(X_train[\"ClaimNb\"]))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
